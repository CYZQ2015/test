{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ori_csv_data = pd.read_csv(os.path.join('sets', '165818_CKD-origin.csv'), encoding='ISO-8859-1')\n",
    "print(\"Data shape (rows, columns) =  \", ori_csv_data.shape)\n",
    "\n",
    "#print(ori_csv_data.info())\n",
    "print(\"Columns: \" + str(list(ori_csv_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# data pre-processing\n",
    "###################################\n",
    "#step 1: remove non label(dCKD) rows\n",
    "dat_proc_1 = ori_csv_data[ori_csv_data['dCKD'].notna()]\n",
    "#print(\"Data shape (rows, columns) =  \", res_dat_proc_1.shape)\n",
    "\n",
    "data_labels = dat_proc_1.dCKD.values\n",
    "print(\"Global TRUE rate: \", data_labels.sum()/len(data_labels))\n",
    "print(\"TRUE count: \", list(dat_proc_1.dCKD).count(1))\n",
    "print(\"FALSE count: \", list(dat_proc_1.dCKD).count(0))\n",
    "\n",
    "#step 2: choose relevant columns\n",
    "dat_proc_2 = dat_proc_1[[ 'age', 'height', 'weight', 'rLen', 'rShort', 'rPT', 'rPTLPA', 'LE', 'rKME', 'rKUPE', 'rRPE ', 'lPT', 'lPGA', 'lTSPA', 'lSPA', 'lPSA', 'lCKE', 'lRUPE', 'lRPE', 'DSK', 'DLK', 'DSR', 'DLR']]\n",
    "#dat_proc_2 = dat_proc_1[[ 'age', 'height', 'weight', 'rLen', 'rPT', 'rPTLPA', 'rKME', 'rKUPE', 'rRPE ', 'lPT', 'lPGA', 'lTSPA', 'lPSA', 'lCKE', 'lRUPE', 'lRPE', 'DSK', 'DLK']]\n",
    "#dat_proc_2 = dat_proc_1[[ 'age', 'height', 'weight', 'rLen', 'rShort', 'rPT', 'rPTLPA', 'LE', 'rKME', 'rKUPE', 'rRPE ', 'lPT', 'lPGA', 'lTSPA', 'lSPA', 'lPSA', 'lCKE', 'lRUPE', 'lRPE', 'DSK', 'DLK']]\n",
    "#dat_proc_2 = dat_proc_1[[ 'age', 'height', 'weight', 'rLen', 'rShort', 'rPT', 'rPTLPA', 'LE', 'rKME', 'rKUPE', 'rRPE ', 'lPT', 'lPGA', 'lTSPA', 'lSPA', 'lPSA', 'lCKE', 'lRUPE', 'lRPE', 'DSR', 'DLR']]\n",
    "#print(\"Data shape (rows, columns) =  \", dat_proc_2.shape)\n",
    "#print(dat_proc_2.count())\n",
    "\n",
    "#step3-6: replace illigal data to median \n",
    "dat_proc_3 = dat_proc_2.replace('#DIV/0!', 0)\n",
    "dat_proc_4 = dat_proc_3.fillna(0)\n",
    "dat_proc_5 = dat_proc_4.astype('float32')\n",
    "dat_proc_6 = dat_proc_5.replace(0, dat_proc_5.median())\n",
    "#print(dat_proc_6.head(10))\n",
    "\n",
    "#step 7: normalization\n",
    "# max-min normalization\n",
    "data = (dat_proc_6 - dat_proc_6.min())/(dat_proc_6.max() - dat_proc_6.min())\n",
    "#print(data.head(10))\n",
    "\n",
    "# z-score normalization\n",
    "data1 = (dat_proc_6 - dat_proc_6.mean())/(dat_proc_6.std())  \n",
    "#print(data1.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for f in list(dat_proc_6.columns):\n",
    "    print(\"\"+str(f)+\":\"+str(np.unique(dat_proc_6[f])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list(data.columns):\n",
    "    print(\"\"+str(f)+\":\"+str(np.unique(data[f])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list(data1.columns):\n",
    "    print(\"\"+str(f)+\":\"+str(np.unique(data1[f])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib as mpl  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc(labels, predict_prob):\n",
    "    false_positive_rate,true_positive_rate,thresholds=roc_curve(labels, predict_prob)\n",
    "    roc_auc=auc(false_positive_rate, true_positive_rate)\n",
    "    plt.title('ROC')\n",
    "    plt.plot(false_positive_rate, true_positive_rate,'b',label='AUC = %0.4f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Split dataset to training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data1, data_labels, test_size=0.5, random_state=10)\n",
    "print(\"training data shape (rows, columns) =  \", train_data.shape)\n",
    "print(\"TRUE rate: \", train_labels.sum()/len(train_labels))\n",
    "\n",
    "print(\"testing data shape (rows, columns) =  \", test_data.shape)\n",
    "print(\"TRUE rate: \", test_labels.sum()/len(test_labels))\n",
    "\n",
    "X = train_data\n",
    "y = train_labels\n",
    "\n",
    "rf0 = RandomForestClassifier(oob_score=True, random_state=10)\n",
    "rf0.fit(X,y)\n",
    "print(rf0.oob_score_)\n",
    "\n",
    "y_predprob = rf0.predict_proba(X)[:,1]\n",
    "\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y,y_predprob))\n",
    "\n",
    "plot_roc(y, y_predprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV # old version from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#n_estimators\n",
    "param_test1= {'n_estimators':list(range(30,201,20))}\n",
    "gsearch1= GridSearchCV(estimator = RandomForestClassifier(min_samples_split=10,\n",
    "                                 min_samples_leaf=20,max_depth=10,max_features='sqrt' ,random_state=10),\n",
    "                       param_grid =param_test1, scoring='roc_auc',cv=6, n_jobs=-1)\n",
    "gsearch1.fit(X,y)\n",
    "gsearch1.cv_results_,gsearch1.best_params_, gsearch1.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth and min_samples_split\n",
    "param_test2= {'max_depth':list(range(4,15,2)), 'min_samples_split':list(range(10,61,10))}\n",
    "gsearch2= GridSearchCV(estimator = RandomForestClassifier(n_estimators=150,min_samples_leaf=20,max_features='sqrt',\n",
    "                                                          oob_score=True,random_state=10),\n",
    "                       param_grid = param_test2,scoring='roc_auc',iid=False, cv=6, n_jobs=-1)\n",
    "gsearch2.fit(X,y)\n",
    "gsearch2.cv_results_,gsearch2.best_params_, gsearch2.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "rf1= RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_split=10,\n",
    "                            min_samples_leaf=20, max_features='sqrt' ,oob_score=True,random_state=10)\n",
    "rf1.fit(test_data,test_labels)\n",
    "print(rf1.oob_score_)\n",
    "\n",
    "y_predprob = rf1.predict_proba(X)[:,1]\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y,y_predprob))\n",
    "plot_roc(y, y_predprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_samples_split and min_samples_leaf\n",
    "param_test3= {'min_samples_split':list(range(2,21,2)), 'min_samples_leaf':list(range(10,61,10))}\n",
    "gsearch3= GridSearchCV(estimator = RandomForestClassifier(n_estimators=150,max_depth=8,\n",
    "                                                          max_features='sqrt',oob_score=True,random_state=10),\n",
    "                       param_grid = param_test3,scoring='roc_auc',iid=False, cv=6, n_jobs=-1)\n",
    "gsearch3.fit(X,y)\n",
    "gsearch3.cv_results_,gsearch3.best_params_, gsearch3.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_features:\n",
    "param_test4= {'max_features':list(range(5,24,2))}\n",
    "gsearch4= GridSearchCV(estimator = RandomForestClassifier(n_estimators=150,max_depth=8, min_samples_split=5,\n",
    "                                 min_samples_leaf=10 ,oob_score=True, random_state=10),\n",
    "   param_grid = param_test4,scoring='roc_auc',iid=False, cv=6, n_jobs=-1)\n",
    "gsearch4.fit(X,y)\n",
    "gsearch4.cv_results_,gsearch4.best_params_, gsearch4.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2= RandomForestClassifier(n_estimators=90, max_depth=10, min_samples_split=25,\n",
    "                                 min_samples_leaf=10,max_features=5 ,oob_score=True, random_state=10)\n",
    "rf2.fit(test_data,test_labels)\n",
    "print(rf2.oob_score_)\n",
    "\n",
    "y_predprob = rf2.predict_proba(X)[:,1]\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y,y_predprob))\n",
    "plot_roc(y, y_predprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
